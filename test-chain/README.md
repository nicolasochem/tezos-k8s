# Notes on current iteration of testnet

## Storage
- Not using manual storage class for persistent volume. Letting minikube manually provision storage.

## ConfigMap
- Set the bootstrap_peers field in the configMap's config.json to "tezos-p2p:9732". That is the service for the peer to peer network. Allows nodes to find each other.
- The config.json and parameters.json have some required values filled that are populated later by a container in the deployment process.

## Deployment
- Launches a progenitor tezos node which which also will run as a baker.
- Before launching, genesis and baker keys are generated. `kubectl patch` is then run to update the configMap after using `sed` with regex to place the keys in the appropriate spots in the file.

## Role and RoleBinding
- A role and role binding are required in order that a container can run the necessary `kubectl` commands against the k8s api.

## activate-job
- Job waits for the progenitor tezos node to be running using `kubectl wait`. Doing so allows the job containers to access the updated configMap with the dynamically generated keys and run the baker command.

## StatefulSet
- Using a statefulSet to launch tezos peers. Each pod waits for the progenitor node to start which makes sure that the configMap was updated with the dynamically generated keys.
- Multiple pods can be launched at the same time in parallel.
- Each pod gets its own provisioned storage in contrast to how a deployment works where they share the same volumes.
- Pods communicate over the tezos-p2p service on port 9732.

# Local Deployment
- To spin up a local cluster, run `kubectl apply -f test-chain/testnet.yml`.

# Expose Cluster Locally on your LAN
Make sure you have `jq` installed on your machine.
- On one machine run the `startAndExposeCluster.sh` script. This does a few things:
  - Enables the minikube nginx ingress addon.
  - Configures the ngnix ingress to handle TCP connections to the tezos-p2p service.
  - Starts up the tezos cluster if it isn't already running (`kubectl apply -f test-chain/testnet.yml`).
  - Logs out the genesis key.
  - Updates the `test-chain/peerCluster.yml` file by adding your machine's ip as a bootstrap peer and setting the genesis_pubkey to the key generated by the tezos node.
  - Port forwards ports 8732 and 9732 from your local machine into minikube.
- Confirm ConfigMap in `test-chain/peerCluster.yml` has your current host's ip in the bootstrap-peers field as well as has set the genesis_pubkey. Manually set these things if they were not set/updated properly. (The script is using `sed` so you may have made changes to the file that `sed` isn't set up to handle.)
- Run the updated `test-chain/peerCluster.yml` file on another machine: `kubectl apply -f peerCluster.yml`
- On your second machine, you should see a StatefulSet spin up with one node. Check its logs to see if it connected to your first machine's cluster. Sometimes it may take a little bit for the connections to be established.

On your second machine you can run `kubectl scale statefulset -n tqtezos1 --replicas=3 tezos-p2p` to spin up more replicas. Notice that they connect to each other on the same machine but only to the bootstrap node on your first machine. Not to any other node you have running on your first machine.

Running `curl $(minikube ip)/network/connections | python -m json.tool | grep -E 'addr|peer_id'` on your first machine, notice all the peers connected to the bootstrap node but that a bunch of ip addresses are the same. These are the nodes from the second machine that I believe are being NATed (investigating what is going on exactly).

## Helpful Commands
- You can make RPC calls from your second machine to the one port forwarding: e.g. `curl PORT_FORWARDING_HOSTS_IP:8732/network/connections`
- You can make RPC calls from the bootstrap node's host itself: `curl $(minikube ip)/network/connections`
- Spin up more StatefulSet replicas: `kubectl scale statefulset -n tqtezos1 --replicas=2 tezos-p2p`
- View logs of tezos nodes: `kubectl logs -n tqtezos1 tezos-p2p-0`
- View logs of nginx ingress controller: `kubectl logs -n kube-system ingress-nginx-controller-SALT -c controller`
